akka {
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    provider = remote
    default-mailbox.stash-capacity = 20000
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    # log-remote-lifecycle-events = off

    # 默认的tcp配置，实际上运行时，会根据fetcher的配置覆盖部分默认参数，如hostname，port
    netty.tcp {
      hostname = "127.0.0.1" #表示绑定当前主机的IP地址
      port = 0   # LISTEN on tcp port， 0 表示动态绑定端口
      message-frame-size = 30000000b
      send-buffer-size = 30000000b
      receive-buffer-size = 30000000b
      maximum-frame-size = 30000000b
    }
  }
}

master {
  bucket {
    maxSize = 2000  #每个桶内最大的链接数量
    fillSize = 2000 #每次注入的链接数量
    # fillSize = 10000 #每次注入的链接数量， 测试用数据
    picker = "simple" #桶选择器，默认为简单选择方式，可以为Simple或者advanced
  }

  robinCount = 5 # 如果大于1，则Master启用RoundRobinPool，否则仅实例化一个FetchMaster

  db {
    path = "./db" #数据库主目录
    cacheSize = 2500 #LRUCache的大小
  }

  hostname = "127.0.0.1" # Master所在的主机名称，在跨网络运行时，可以通过该名称访问到master
  port = 2552 # Master绑定的端口号

  link.max.depth = 2 # 对于抽取出的子链接(种子链接的默认深度为0)，保存的最大深度, 超过该值的子链接将抛弃
  link.max.retries = 5 # 最大重试次数, 如果超过该数值，则把该链接扔掉。

  http.port = 7000 # Web API restful service port
}

fetcher {
  user.agent = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/52.0.2743.116 Chrome/52.0.2743.116 Safari/537.36" # 模拟浏览器的userAgent名称

  hostname = "127.0.0.1" # Fetcher所在的主机名称，在跨网络运行时，可以通过该名称访问到fetcher

  #actor count of each fetcher node
  numOfFetchClientActors = 13

  #是否解析数据页面的子连接
  parseDataPageLinks = false

  //符合该URL类型的链接，会进一步获取其跳转后的地址，作为文章链接进行后续处理
  jumpingUrls = [
    "http(s|)?://www.baidu.com/link\\?url=.+",
    "https://www.bing.com/link\\?url=.+"
  ]

  //需要抽取出子链接的规则处理，例如http://a.com/url?url=http://b.com/123.html
  //需要把该地址变为http://b.com/123.html
  //https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=newssearch&cd=3&cad=rja&uact=8&ved=0ahUKEwiJnPyi9OTUAhWOZj4KHZSHBUoQqQIIKygAMAI&url=https%3A%2F%2Fwww.voachinese.com%2Fa%2Fnews-investigation-launched-into-overseas-investment-by-companies-allegedly-related-to-top-leaders-20170622%2F3911671.html&usg=AFQjCNGZ_0IfBMQdI-KqwvPR7EcxfM-uzg
  paramUrls = [
    {
      //符合url正则表达式的会进行抽取该url中包含的参数对应的参数值
      //该值作为结果保留
      url = "https://www.google.com/url\\?.+"
      param = "url" //参数里面的url是需要抽取出来的结果
      encoding = "utf-8"
    },
    {
      //符合url正则表达式的会进行抽取该url中包含的参数对应的参数值
      //该值作为结果保留
      url = "https://www.bing.com/url\\?.+"
      param = "url" //参数里面的url是需要抽取出来的结果
    }
  ]
}

# 白名单和黑名单过滤机制
article {
  mustContainsRegex = "" # 只有文章标题或内容符合该正则表达式才会入库，空串表示所有文章都可以接收
  checkDuplicate = false # 是否在文章入库前，进行URL重复检测，加上此步操作会降低系统性能, 因此，默认不启用
  saveHtmlFormat = true # 在保存时是否保存带格式的正文
}

# 调度机制，如每日发送邮件
scheduler {
  mail {
    triggerTimes = "1:00 4:00 7:00 10:00 13:00 16:00 19:00 22:00"

    smtp {
      host = "smtp.163.com"
      port = 25
      user = "user@163.com"
      password = "your_password"
      auth = true
      startTtls = true
    }

    receivers = "user1@example.com;user2@example.com"
    #是否启用邮件通知，发送最近抓取的报道内容列表，默认为false
    notify = false
  }
}

